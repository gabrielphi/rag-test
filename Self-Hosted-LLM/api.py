import os
import time
import json
import pickle
from contextlib import asynccontextmanager
from typing import List, Optional
import re

# --- FastAPI & Pydantic ---
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, ConfigDict, Field

# --- LangChain Imports ---
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import create_retrieval_chain, create_history_aware_retriever
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage, AIMessage

# --- Otimiza√ß√£o: Re-ranking e MultiQuery ---
from langchain.retrievers import ContextualCompressionRetriever, EnsembleRetriever
from langchain.retrievers.document_compressors import FlashrankRerank
from langchain.retrievers.multi_query import MultiQueryRetriever
from flashrank import Ranker
from langchain_core.output_parsers import JsonOutputParser

# --- Configura√ß√µes ---
from dotenv import load_dotenv

# Carrega as vari√°veis de ambiente
load_dotenv()

# --- Constantes ---
VECTOR_DB_FOLDER = os.getenv("VECTOR_DB_FOLDER", "vector_db")
BM25_INDEX_FILE = os.getenv("BM25_INDEX_FILE", "bm25_index.pkl")
EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
LLM_MODEL = os.getenv("LLM_MODEL")
RERANK_MODEL_NAME = os.getenv("RERANK_MODEL_NAME", "ms-marco-MiniLM-L-12-v2")
MAX_HISTORY_MESSAGES = 6


# --- Vari√°veis Globais (Componentes Reutiliz√°veis) ---
# N√£o guardamos mais a chain pronta, mas sim as pe√ßas para mont√°-la
vectorstore_global = None
keyword_retriever_global = None
llm_global = None
reranker_global = None
qa_prompt_global = None
contextualize_prompt_global = None
VALID_TOPICS = set() # Cache de t√≥picos v√°lidos (arquivos/pastas)


# --- Modelos Pydantic ---

class Message(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[Message]
    stream: Optional[bool] = False
    temperature: Optional[float] = None
    model_config = ConfigDict(extra="ignore")

class Usage(BaseModel):
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0

class Choice(BaseModel):
    index: int
    message: Message
    finish_reason: Optional[str] = "stop"

class ChatCompletionResponse(BaseModel):
    id: str
    object: str
    created: int
    model: str
    choices: List[Choice]
    usage: Usage

# --- NOVO: Modelo de Inten√ß√£o de Busca ---
class SearchIntent(BaseModel):
    entities: List[str] = Field(
        default=[], 
        description="Nomes espec√≠ficos, Projetos, Produtos, Lugares ou Arquivos. Ex: ['Projeto Alpha', 'Relat√≥rio Anual']"
    )
    topics: List[str] = Field(
        default=[], 
        description="Temas gerais, conceitos abstratos. Ex: ['Marketing', 'Seguran√ßa', 'Vendas']"
    )
    is_global_query: bool = Field(
        default=False, 
        description="True se a pergunta for gen√©rica demais para filtrar (ex: 'Como jogar?', 'Liste todas as ra√ßas')."
    )
    # --- NOVO: Suporte a Prioriza√ß√£o de √çndices ---
    requires_index: bool = Field(
        default=False,
        description="True se o usu√°rio pede uma LISTA, RESUMO ou VIS√ÉO GERAL. Ex: 'Quais ra√ßas existem?', 'Resuma o sistema'."
    )
    context_filter: Optional[str] = Field(
        default=None,
        description="Filtro de contexto inferido: 'Ra√ßa', 'Classe', 'Regra' ou None."
    )
    model_config = ConfigDict(extra="ignore") # Blindagem: Ignora campos extras alucinados pelo LLM

# --- Fun√ß√µes de L√≥gica RAG ---

# --- Fun√ß√µes de L√≥gica RAG ---

def normalize_topic_name(name: str) -> str:
    """
    Normaliza nome de arquivo/pasta para compara√ß√£o fuzzy.
    Ex: '00_INDICE_MUNDO.txt' -> '00 indice mundo'
    Ex: 'era_das_bestas' -> 'era das bestas'
    """
    # Remove extens√£o
    name = os.path.splitext(name)[0]
    # Substitui underscore e h√≠fens por espa√ßo
    name = name.replace("_", " ").replace("-", " ")
    return name.lower().strip()

def load_valid_topics():
    """
    Escaneia a pasta de documentos e popula o set VALID_TOPICS.
    """
    global VALID_TOPICS
    VALID_TOPICS.clear()
    
    if not os.path.exists("documentos"):
        print("‚ö†Ô∏è Pasta 'documentos' n√£o encontrada para indexa√ß√£o de t√≥picos.")
        return

    print("üìÇ Indexando t√≥picos v√°lidos...")
    count = 0
    for root, dirs, files in os.walk("documentos"):
        # Indexa nomes de pastas
        for d in dirs:
            normalized = normalize_topic_name(d)
            VALID_TOPICS.add(normalized)
            count += 1
            
        # Indexa nomes de arquivos
        for f in files:
            normalized = normalize_topic_name(f)
            VALID_TOPICS.add(normalized)
            count += 1
            
    print(f"‚úÖ {count} t√≥picos v√°lidos indexados.")

async def extract_search_intent(query: str, llm) -> SearchIntent:
    """
    Vers√£o Blindada: Usa Regex para limpar markdown e for√ßa JSON puro.
    Resistente a 'chatice' do modelo (introdu√ß√µes, repeti√ß√µes).
    """
    # 1. Prompt muito mais expl√≠cito e autorit√°rio
    system_prompt = """
    ATEN√á√ÉO: Voc√™ √© um SISTEMA (API), n√£o um assistente de chat.
    Sua √∫nica fun√ß√£o √© converter a query do usu√°rio em um objeto JSON de filtros.
    N√ÉO responda a pergunta. N√ÉO repita a pergunta. N√ÉO explique.
    
    SCHEMA JSON OBRIGAT√ìRIO:
    {{
        "entities": ["SpecificSubject", "ProperNoun", "ProjectName"],
        "topics": ["GeneralTheme", "Concept", "Process"],
        "is_global_query": boolean,
        "requires_index": boolean,
        "context_filter": "CategoryName" | null
    }}

    REGRAS DE OURO:
    1. Se a pergunta for sobre UM ASSUNTO ESPEC√çFICO (Ex: 'Projeto X', 'Cliente Y', 'Arquivo Z'), coloque em 'entities'.
    2. ENTIDADE = Nomes Pr√≥prios, Projetos, Produtos, Arquivos Espec√≠ficos.
    3. Se o usu√°rio quer UMA LISTA ou VIS√ÉO GERAL, marque "requires_index": true.
    
    EXEMPLOS:
    Input: "O que diz a politica de RH?"
    Output: {{"entities": ["Politica de RH"], "topics": [], "is_global_query": false, "requires_index": false, "context_filter": "RH"}}
    
    Input: "Como configurar o ambiente?"
    Output: {{"entities": [], "topics": ["Configurar Ambiente"], "is_global_query": false, "requires_index": false, "context_filter": "Tecnologia"}}
    
    Input: "Liste todos os documentos de marketing"
    Output: {{"entities": [], "topics": [], "is_global_query": false, "requires_index": true, "context_filter": "Marketing"}}

    Input: "Como funciona o combate?"
    Output: {{"entities": [], "topics": ["Combate"], "is_global_query": false, "requires_index": false, "context_filter": "Regra"}}
    """
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "Input: {question}\nOutput JSON:")
    ])
    
    # Invocamos o LLM diretamente (sem o parser na chain para podermos limpar o texto antes)
    chain = prompt | llm
    
    try:
        response_msg = await chain.ainvoke({"question": query})
        
        # O LangChain pode retornar string ou objeto AIMessage dependendo da vers√£o
        content = response_msg.content if hasattr(response_msg, 'content') else str(response_msg)

        # --- DEBUG CR√çTICO: Ver o que o modelo cuspiu ---
        print(f"üïµÔ∏è RAW INTENT RESPONSE: {content[:200]}...") 
        
        # --- LIMPEZA CIR√öRGICA (A "Blindagem") ---
        # 1. Remove blocos de c√≥digo markdown (```json ... ```)
        content = re.sub(r'```json\s*', '', content)
        content = re.sub(r'```', '', content)
        
        # 2. Tenta encontrar onde come√ßa o JSON '{' e onde termina '}'
        start = content.find('{')
        end = content.rfind('}') + 1
        
        if start != -1 and end != -1:
            clean_json = content[start:end]
            # Faz o parse manual
            data = json.loads(clean_json)
            return SearchIntent(**data)
        else:
            raise ValueError("Nenhum JSON encontrado na resposta.")

    except Exception as e:
        print(f"‚ùå ERRO NO PARSER DE INTEN√á√ÉO: {e}")
        print(f"   Conte√∫do que falhou: {content[:100]}...")
        # Fallback: Busca Global
        return SearchIntent(is_global_query=True)

def build_dynamic_rag_chain(intent: SearchIntent):
    """
    Constr√≥i a pipeline RAG.
    CORRE√á√ÉO CR√çTICA: Desativa o BM25 se houver filtros de entidade para evitar vazamento de contexto.
    """
    global vectorstore_global, keyword_retriever_global, llm_global
    global reranker_global, qa_prompt_global, contextualize_prompt_global

    print(f"\nüîß Construindo Chain para: Entities={intent.entities} | Topics={intent.topics}")

    # 1. Configurar Filtros do VectorStore (Chroma)
    # 1. Configurar Filtros Din√¢micos (Chroma)
    # Aumentamos K para garantir que pegamos o documento INTEIRO se poss√≠vel (especialmente se for curto)
    search_kwargs = {"k": 20, "fetch_k": 80}  
    
    filters_list = []
    # has_strict_filter indica se estamos filtrando por UMA entidade espec√≠fica (ex: S√≥ Fej)
    # Se sim, isso nos permite IGNORAR o BM25 para evitar ru√≠do.
    has_strict_filter = False 

    # --- NOVO: L√≥gica de Filtro Aprimorada ---
    
    # A. Prioriza√ß√£o de √çndice (Listagens) - PRIORIDADE M√ÅXIMA
    # Se requer √≠ndice, filtramos por √≠ndice INDEPENDENTE se √© query global ou n√£o.
    if intent.requires_index:
        print("üìë Detectada inten√ß√£o de √çNDICE/LISTAGEM.")
        filters_list.append({"is_index": True})
        if intent.context_filter:
            filters_list.append({"context_type": intent.context_filter})
        
        # Sobrescreve filtro para garantir que pegamos S√ì o √≠ndice
        if len(filters_list) > 1:
            search_kwargs["filter"] = {"$and": filters_list} # Tenta ser restritivo
        else:
             search_kwargs["filter"] = filters_list[0]
        
        print(f"üîí Filtro Chroma (Index): {search_kwargs.get('filter')}")


    # B. Filtros de Entidade/T√≥pico (Apenas se N√ÉO for busca global E n√£o for √≠ndice j√° tratado)
    elif not intent.is_global_query:
        
        # B. Filtro Estrito de Entidade (Evita alucina√ß√£o entre ra√ßas)
        # MAS: Se tiver T√≥picos junto (Ex: "Fej na Ruptura"), √© Cross-Reference. N√ÉO filtrar estrito se tiver t√≥pico v√°lido.
        if intent.entities and not intent.topics:
            print(f"üéØ Entidade Detectada (Foco √önico): {intent.entities[0]}")
            normalized_entity = normalize_topic_name(intent.entities[0])
            
            # CRITICAL FIX: S√≥ aplica filtro estrito se a entidade for um ARQUIVO existente.
            # Caso contr√°rio (ex: "Pedra Viva" que √© uma habilidade dentro de um arquivo), 
            # n√£o filtramos metadata, deixamos o vector search achar no conte√∫do.
            if normalized_entity in VALID_TOPICS:
                primary_entity = intent.entities[0].strip().title()
                filters_list.append({"entity": primary_entity})
                has_strict_filter = True
            else:
                print(f"‚ö†Ô∏è Entidade '{intent.entities[0]}' n√£o √© um arquivo/t√≥pico v√°lido. Entrando em modo 'Busca de Conte√∫do'.")
                # N√£o aplicamos filtro 'entity', o retrieval vai buscar no texto full.

        # C. Valida√ß√£o de T√≥picos e Cross-Reference
        elif intent.topics:
            # Separa t√≥picos v√°lidos (existem no disco) e inv√°lidos
            valid_topics_found = []
            invalid_topics_found = []

            for topic in intent.topics:
                normalized = normalize_topic_name(topic)
                if normalized in VALID_TOPICS:
                    valid_topics_found.append(topic) # Guarda o original para filtro
                else:
                    invalid_topics_found.append(topic)
            
            # Se tem t√≥picos v√°lidos, aplica filtro estrito DESTE t√≥pico
            if valid_topics_found:
                for topic in valid_topics_found:
                    clean = topic.strip().title()
                    # Tenta ser flex√≠vel: Source ou Category
                    filters_list.append({"source": clean})
                    filters_list.append({"category": clean})
                print(f"‚úÖ T√≥picos V√°lidos Filtrados: {valid_topics_found}")
            
            # Se tem t√≥picos INV√ÅLIDOS...
            if invalid_topics_found:
                print(f"‚ö†Ô∏è T√≥picos Inv√°lidos (Sem arquivo correspondente): {invalid_topics_found}")
                
                # Se TEM entidade E t√≥pico inv√°lido -> INJE√á√ÉO DE "SEM RELA√á√ÉO"
                if intent.entities:
                     # N√ÉO filtramos pelo t√≥pico inv√°lido (para n√£o zerar busca).
                     # Mas avisamos o LLM para checar a rela√ß√£o.
                     print(f"üíâ Injetando contexto de 'Verificar Rela√ß√£o' para: {intent.entities} + {invalid_topics_found}")
                     
                     # Adicionamos um filtro de entidade para garantir que achamos algo sobre a entidade pelo menos
                     primary_entity = intent.entities[0].strip().title()
                     filters_list.append({"entity": primary_entity})
                     
                     # A m√°gica acontece no Prompt do LLM, mas aqui garantimos que o retriever traga dados da entidade
                     # para o LLM poder dizer "Isso √© sobre Fej, mas n√£o achei nada sobre Terra do Nunca aqui."
                else:
                    # Se S√ì tem t√≥pico inv√°lido e nenhuma entidade... fallback para busca gen√©rica total (sem filtro)
                    print("‚ö†Ô∏è Apenas t√≥pico inv√°lido detectado. Fallback para busca sem√¢ntica aberta.")
                    pass

        # Monta o filtro final do Chroma
        if filters_list:
            if len(filters_list) > 1:
                # Se tem v√°rios crit√©rios, usa OR 
                if intent.requires_index:
                     search_kwargs["filter"] = {"is_index": True}
                else:
                     search_kwargs["filter"] = {"$or": filters_list}
            else:
                search_kwargs["filter"] = filters_list[0]
            
            print(f"üîí Filtro Chroma: {search_kwargs.get('filter')}")

    # 2. Criar Retriever Vetorial
    vector_retriever = vectorstore_global.as_retriever(
        search_type="mmr",
        search_kwargs=search_kwargs
    )

    # 3. Sele√ß√£o de Estrat√©gia de Busca
    # "Fa√ßa com que o c√≥digo sempre utilize a melhor maneira, se precisar ignorar o BM25, pode fazer."
    
    if has_strict_filter:
        print("üö´ ESTRAT√âGIA: VECTOR ONLY (Strict). BM25 Desativado para evitar polui√ß√£o de contexto.")
        base_retriever = vector_retriever
    elif intent.requires_index:
        print("üìë ESTRAT√âGIA: VECTOR ONLY (Index Focus). Focando em metadados de √≠ndice.")
        base_retriever = vector_retriever
    else:
        # Busca aberta/tem√°tica: BM25 ajuda a achar termos exatos no meio de textos grandes
        if keyword_retriever_global:
            print("‚úÖ ESTRAT√âGIA: HYBRID (Vector + BM25). Melhor para buscas tem√°ticas ou globais.")
            base_retriever = EnsembleRetriever(
                retrievers=[keyword_retriever_global, vector_retriever],
                weights=[0.4, 0.6]
            )
        else:
            base_retriever = vector_retriever

    # 4. MultiQuery (Opcional - Pode comentar se quiser mais velocidade)
    # √Äs vezes o MultiQuery tamb√©m alucina termos. Vamos manter mas com aten√ß√£o.
    
    # OTIMIZA√á√ÉO: Se for busca de √çNDICE, n√£o queremos varia√ß√µes. Queremos o √≠ndice.
    if intent.requires_index:
        print("‚è© Pulo MultiQuery para busca de √çndice (Foco na exatid√£o).")
        multi_query_retriever = base_retriever 
    else:
        mq_prompt = ChatPromptTemplate.from_messages([
            ("system", 
            "Voc√™ √© um assistente de busca. Reescreva a pergunta em 3 varia√ß√µes simples para encontrar a resposta no banco de dados."
            "Voc√™ est√° PROIBIDO de tentar achar novos contextos, citando ferramentas ou documentos ou franquias que voc√™ n√£o recebeu."),
            ("human", "{question}")
        ])
        multi_query_retriever = MultiQueryRetriever.from_llm(
            retriever=base_retriever,
            llm=llm_global,
            prompt=mq_prompt
        )

    # 5. Reranker (Flashrank)
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=reranker_global, 
        base_retriever=multi_query_retriever 
    )

    # 6. History Aware
    history_aware_retriever = create_history_aware_retriever(
        llm_global, compression_retriever, contextualize_prompt_global
    )

    # 7. Chain Final
    
    # 7. Chain Final
    # L√≥gica de Prompt Din√¢mico (Inje√ß√£o de Aviso)
    final_qa_prompt = qa_prompt_global
    
    # Se detectamos necessidade de aviso na valida√ß√£o de t√≥picos...
    # (Precisamos passar essa info da valida√ß√£o para c√°, vamos usar uma variavel local captured)
    # Re-executando a verifica√ß√£o localmente pois o 'filters_list' j√° foi processado
    
    # Maneira mais limpa: Vamos capturar a mensagem de inje√ß√£o durante a valida√ß√£o
    injection_msg = ""
    if not intent.is_global_query and intent.topics:
        # Re-check simples ou podemos ter salvo numa var local acima.
        # Vamos confiar que se 'filters_list' tem 'entity' mas n√£o tem o t√≥pico (pq era inv√°lido),
        # podemos inferir ou melhor: fazer a l√≥gica de "warning" ser expl√≠cita.
        
        # Recalculando rapidinho para ter certeza (overhead desprez√≠vel)
        invalid_topics_found = [t for t in intent.topics if normalize_topic_name(t) not in VALID_TOPICS]
        
        if invalid_topics_found and intent.entities:
             injection_msg = (
                 f"\n\nATEN√á√ÉO DO SISTEMA: O usu√°rio mencionou o t√≥pico '{invalid_topics_found[0]}' que N√ÉO consta na base de dados. "
                 f"Se voc√™ encontrar informa√ß√µes sobre '{intent.entities[0]}', mas nada que o ligue a '{invalid_topics_found[0]}', "
                 f"AVISE O USU√ÅRIO explicitamente: 'Encontrei informa√ß√µes sobre {intent.entities[0]}, mas n√£o h√° registros relacionando-o com {invalid_topics_found[0]}'."
             )

    if injection_msg:
        print(f"üíâ Criando prompt customizado com aviso: {injection_msg}")
        # Cria um novo prompt template baseada no global + aviso
        system_msg = (
             "Voc√™ √© um assistente de Base de Conhecimento Especializado. Use o CONTEXTO abaixo para responder."
             "Responda SEMPRE em Portugu√™s."
             "Voc√™ √© um assistente estrito de Base de Conhecimento. Use EXCLUSIVAMENTE o contexto fornecido."
             "PROIBIDO usar conhecimentos externos ou de outras franquias"
             "Se o contexto tiver tags [DOC: X], respeite a fonte."
             "Se n√£o souber, diga 'N√£o consta nos documentos'."
             "N√ÉO utilize seu conhecimento pr√©vio sobre jogos, filmes ou livros."
             f"{injection_msg}"
             "\n\nCONTEXTO:\n{context}"
        )
        final_qa_prompt = ChatPromptTemplate.from_messages([
            ("system", system_msg),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ])

    question_answer_chain = create_stuff_documents_chain(llm_global, final_qa_prompt)
    
    return create_retrieval_chain(history_aware_retriever, question_answer_chain)
# --- Lifespan (Inicializa√ß√£o dos Componentes) ---

@asynccontextmanager
async def lifespan(app: FastAPI):
    global vectorstore_global, keyword_retriever_global, llm_global
    global reranker_global, qa_prompt_global, contextualize_prompt_global


    print("\nüöÄ Inicializando Componentes RAG (Modo Din√¢mico)...")

    # 0. Carrega T√≥picos V√°lidos
    load_valid_topics()

    if not os.path.exists(VECTOR_DB_FOLDER):
        print(f"‚ùå Erro: Pasta '{VECTOR_DB_FOLDER}' n√£o encontrada.")
        yield
        return

    # 1. Componentes Pesados (Carregados 1 vez na mem√≥ria)
    print("üîπ Carregando Embeddings e ChromaDB...")
    embedding_function = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    vectorstore_global = Chroma(persist_directory=VECTOR_DB_FOLDER, embedding_function=embedding_function)

    # 2. BM25
    if os.path.exists(BM25_INDEX_FILE):
        print("üíæ Carregando BM25...")
        with open(BM25_INDEX_FILE, "rb") as f:
            keyword_retriever_global = pickle.load(f)
    else:
        # Se n√£o tiver BM25, criamos na hora (simplified fallback)
        print("‚ö†Ô∏è Aviso: BM25 index n√£o encontrado. A busca ser√° apenas vetorial neste boot.")
        keyword_retriever_global = None

    # 3. LLM e Reranker
    print("üîπ Inicializando LLM e Reranker...")
    llm_global = OllamaLLM(model=LLM_MODEL, temperature=0.0, num_ctx=16384)
    
    flashrank_client = Ranker(model_name=RERANK_MODEL_NAME, cache_dir="flashrank_cache")
    # Relaxamos o threshold para 0.01 para n√£o descartar informa√ß√£o √∫til, apenas reordenar.
    reranker_global = FlashrankRerank(client=flashrank_client, top_n=10, score_threshold=0.01)

    # 4. Defini√ß√£o dos Prompts (Fixos)
    contextualize_prompt_global = ChatPromptTemplate.from_messages([
        ("system", (
            "Reformule a pergunta do usu√°rio para ser autossuficiente."
            "Ignore respostas anteriores de 'n√£o sei' ou erros."
            "Retorne APENAS a pergunta reformulada em Portugu√™s."
        )),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    
    qa_prompt_global = ChatPromptTemplate.from_messages([
        ("system", (
            "Voc√™ √© um assistente de Base de Conhecimento Especializado. Use o CONTEXTO abaixo para responder."
            "Responda SEMPRE em Portugu√™s."
            "Voc√™ √© um assistente estrito de Base de Conhecimento. Use EXCLUSIVAMENTE o contexto fornecido."
            "PROIBIDO usar conhecimentos externos ou de outras franquias. N√ÉO invente informa√ß√µes."
            "Se o contexto estiver vazio ou n√£o contiver a resposta, diga EXATAMENTE e APENAS: 'N√£o consta nos documentos'."
            "Se o contexto tiver tags [DOC: X], respeite a fonte."
            "N√ÉO utilize seu conhecimento pr√©vio sobre jogos, filmes ou livros."
            "\n\nCONTEXTO:\n{context}"
        )),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])

    print("‚úÖ Sistema Pronto! Chains ser√£o montadas sob demanda.")
    yield
    print("üõë Desligando...")

app = FastAPI(title="Dynamic RPG RAG API", lifespan=lifespan)

# --- Fun√ß√µes de Stream ---

async def generate_stream(query: str, chat_history: List, model: str, dynamic_chain):
    """
    Fun√ß√£o geradora que usa a chain din√¢mica criada para este request.
    """
    request_id = f"chatcmpl-{int(time.time())}"
    created_time = int(time.time())

    yield f"data: {json.dumps({'id': request_id, 'object': 'chat.completion.chunk', 'created': created_time, 'model': model, 'choices': [{'index': 0, 'delta': {'role': 'assistant'}, 'finish_reason': None}]})}\n\n"

    sources_text = ""
    try:
        # Usa a chain passada como argumento
        async for chunk in dynamic_chain.astream({"input": query, "chat_history": chat_history}):
            if 'answer' in chunk and chunk['answer']:
                yield f"data: {json.dumps({'id': request_id, 'object': 'chat.completion.chunk', 'created': created_time, 'model': model, 'choices': [{'index': 0, 'delta': {'content': chunk['answer']}, 'finish_reason': None}]})}\n\n"
            
            if 'context' in chunk:
                for doc in chunk['context']:
                    src = doc.metadata.get('source', 'unknown')
                    # Tenta pegar p√°gina ou se√ß√£o se existir
                    loc = doc.metadata.get('Header 1') or doc.metadata.get('page', '?')
                    entry = f"{src} ({loc})"
                    if entry not in sources_text: 
                        sources_text += f"\n- {entry}"
                        
    except Exception as e:
        error_msg = f"[Erro no processamento: {str(e)}]"
        yield f"data: {json.dumps({'id': request_id, 'object': 'chat.completion.chunk', 'created': created_time, 'model': model, 'choices': [{'index': 0, 'delta': {'content': error_msg}, 'finish_reason': None}]})}\n\n"

    if sources_text:
        yield f"data: {json.dumps({'id': request_id, 'object': 'chat.completion.chunk', 'created': created_time, 'model': model, 'choices': [{'index': 0, 'delta': {'content': f'\n\n**Fontes:**{sources_text}'}, 'finish_reason': None}]})}\n\n"

    yield f"data: {json.dumps({'id': request_id, 'object': 'chat.completion.chunk', 'created': created_time, 'model': model, 'choices': [{'index': 0, 'delta': {}, 'finish_reason': 'stop'}]})}\n\n"
    yield "data: [DONE]\n\n"

# --- Endpoint Principal ---

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    if not vectorstore_global:
        raise HTTPException(status_code=503, detail="Sistema iniciando...")

    raw_messages = request.messages
    query = raw_messages[-1].content
    
    # 1. Tratamento de Hist√≥rico (Remove erros anteriores)
    relevant_history = raw_messages[:-1][-MAX_HISTORY_MESSAGES:]
    chat_history = []
    
    for msg in relevant_history:
        if msg.role == "user":
            chat_history.append(HumanMessage(content=msg.content))
        elif msg.role == "assistant":
            # Filtro de Toxicidade de Contexto
            content_lower = msg.content.lower()
            if any(x in content_lower for x in ["n√£o consta", "desculpe", "n√£o encontrei"]):
                continue 
            
            content_clean = msg.content.split("**Fontes:**")[0].strip()
            chat_history.append(AIMessage(content=content_clean))

    # 2. Detec√ß√£o de Inten√ß√£o (Router)
    # Descobre se precisa filtrar por "Fej", "Hist√≥ria", etc.
    print(f"ü§î Analisando inten√ß√£o para: '{query}'")
    search_intent = await extract_search_intent(query, llm_global)
    
    # 3. Montagem da Chain Din√¢mica
    current_chain = build_dynamic_rag_chain(search_intent)

    # 4. Execu√ß√£o (Stream ou Invoke)
    if request.stream:
        return StreamingResponse(
            generate_stream(query, chat_history, request.model, current_chain), 
            media_type="text/event-stream"
        )

    # Execu√ß√£o normal (Non-stream)
    response = await current_chain.ainvoke({
        "input": query,
        "chat_history": chat_history
    })
    
    answer_content = response['answer']
    if 'context' in response:
        sources = set([f"{doc.metadata.get('source', 'Doc')} ({doc.metadata.get('Header 1', '')})" for doc in response['context']])
        if sources:
            answer_content += "\n\n**Fontes:**\n- " + "\n- ".join(sources)

    return ChatCompletionResponse(
        id=f"chatcmpl-{int(time.time())}",
        object="chat.completion",
        created=int(time.time()),
        model=request.model,
        choices=[Choice(index=0, message=Message(role="assistant", content=answer_content))],
        usage=Usage()
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)